{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ======================================================\n",
    "# 0) Load and clean full dataset, then sample 200 rows and plot distribution\n",
    "# ======================================================\n",
    "import os\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "RNG = np.random.RandomState(42)\n",
    "\n",
    "CLEAN_SOURCE = 'debug_cleaned.csv'\n",
    "RAW_SOURCE = '../data_files/dehum_data_explicit.csv'\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Helper: strip junk, sniff delimiter, rebuild rows permissively\n",
    "# ------------------------------------------------------\n",
    "def load_and_reconstruct(path):\n",
    "    print(f\"Loading raw text from {path}\")\n",
    "    with open(path, 'r', encoding='utf-8', errors='replace') as fh:\n",
    "        raw = fh.read()\n",
    "\n",
    "    raw = raw.replace(';;;;;;;;;;;;;;;;;;;;;;', '')\n",
    "    raw = re.sub(r\";+\\s*$\", \"\", raw, flags=re.MULTILINE)\n",
    "\n",
    "    lines = raw.splitlines()\n",
    "    if not lines:\n",
    "        raise ValueError(\"File is empty after cleaning\")\n",
    "\n",
    "    sample = '\\n'.join(lines[:50])\n",
    "    try:\n",
    "        delim = csv.Sniffer().sniff(sample, delimiters=[',', ';', '\\t', '|']).delimiter\n",
    "    except Exception:\n",
    "        delim = ','\n",
    "    print(f\"Detected delimiter: {repr(delim)}\")\n",
    "\n",
    "    header_cols = [c.strip() for c in lines[0].split(delim)]\n",
    "    ncols = len(header_cols)\n",
    "    print(f\"Header columns detected: {ncols}\")\n",
    "\n",
    "    reconstructed = []\n",
    "    buffer_lines = []\n",
    "    expected_seps = ncols - 1\n",
    "\n",
    "    def flush_buffer(buf):\n",
    "        chunk = '\\n'.join(buf)\n",
    "        try:\n",
    "            parsed = next(csv.reader([chunk], delimiter=delim, quotechar='\"'))\n",
    "            if len(parsed) == ncols:\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            parsed_alt = next(csv.reader([chunk.replace('\"', \"'\"], delimiter=delim, quotechar=\"'\")))\n",
    "            if len(parsed_alt) == ncols:\n",
    "                return parsed_alt\n",
    "        except Exception:\n",
    "            pass\n",
    "        pieces = chunk.split(delim)\n",
    "        if len(pieces) < ncols:\n",
    "            pieces.extend([''] * (ncols - len(pieces)))\n",
    "        elif len(pieces) > ncols:\n",
    "            pieces = pieces[:ncols]\n",
    "        return [p.strip() for p in pieces]\n",
    "\n",
    "    current_sep_count = 0\n",
    "    for line in lines[1:]:\n",
    "        buffer_lines.append(line)\n",
    "        current_sep_count += line.count(delim)\n",
    "        if current_sep_count >= expected_seps:\n",
    "            reconstructed.append(flush_buffer(buffer_lines))\n",
    "            buffer_lines = []\n",
    "            current_sep_count = 0\n",
    "\n",
    "    if buffer_lines:\n",
    "        reconstructed.append(flush_buffer(buffer_lines))\n",
    "        print(\"Warning: leftover buffer flushed with permissive split.\")\n",
    "\n",
    "    print(f\"Reconstructed rows: {len(reconstructed)}\")\n",
    "    df = pd.DataFrame(reconstructed, columns=[c.strip().lower() for c in header_cols])\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Load data (prefer cleaned source if available)\n",
    "# ------------------------------------------------------\n",
    "source_path = CLEAN_SOURCE if os.path.exists(CLEAN_SOURCE) else RAW_SOURCE\n",
    "print(f\"Primary source selected: {source_path}\")\n",
    "try:\n",
    "    df = pd.read_csv(source_path, sep=',', engine='python', dtype=str)\n",
    "    print(\"Direct pandas read succeeded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Direct read failed: {repr(e)}\")\n",
    "    df = load_and_reconstruct(source_path)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"Initial DataFrame empty; forcing reconstruction.\")\n",
    "    df = load_and_reconstruct(source_path)\n",
    "\n",
    "# Drop unnamed/index-like columns and empty first column\n",
    "df = df.loc[:, [not str(c).lower().startswith('unnamed') for c in df.columns]]\n",
    "if '' in df.columns and df[''].astype(str).str.strip().eq('').all():\n",
    "    df = df.drop(columns=[''])\n",
    "\n",
    "# Persist the cleaned full dataset for reuse\n",
    "full_out = 'clean_full.csv'\n",
    "df.to_csv(full_out, index=False)\n",
    "print(f\"Saved cleaned dataset to {full_out} (rows={len(df)})\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Ensure label column and numeric conversions\n",
    "# ------------------------------------------------------\n",
    "label_cols = ['animal','subhuman','disease','inanimate','agg_animal']\n",
    "if 'exp_dehum_binary' not in df.columns:\n",
    "    present = [c for c in label_cols if c in df.columns]\n",
    "    if present:\n",
    "        print('Inferring exp_dehum_binary from subtype columns:', present)\n",
    "        df[present] = df[present].fillna('0')\n",
    "        df['exp_dehum_binary'] = (df[present].astype(float).sum(axis=1) > 0).astype(int)\n",
    "    elif 'explicit' in df.columns:\n",
    "        print('Using explicit column to approximate exp_dehum_binary.')\n",
    "        df['exp_dehum_binary'] = df['explicit'].astype(str).str.lower().replace({'true':'1','false':'0','yes':'1','no':'0','nan':'0','none':'0','': '0'}).astype(int)\n",
    "    else:\n",
    "        print('No subtype/explicit columns found; setting exp_dehum_binary to 0.')\n",
    "        df['exp_dehum_binary'] = 0\n",
    "\n",
    "df['exp_dehum_binary'] = pd.to_numeric(df['exp_dehum_binary'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Sample 200 rows (or fewer) and plot distribution\n",
    "# ------------------------------------------------------\n",
    "N = 200\n",
    "n_take = min(N, len(df))\n",
    "if n_take < N:\n",
    "    print(f\"Dataset has only {len(df)} rows; sampling all available rows.\")\n",
    "sample_df = df.sample(n=n_take, random_state=RNG).reset_index(drop=True)\n",
    "\n",
    "sample_df['dehumanized_label'] = sample_df['exp_dehum_binary'].map({1: 'dehumanized', 0: 'not_dehumanized'})\n",
    "counts = sample_df['dehumanized_label'].value_counts().reindex(['dehumanized','not_dehumanized']).fillna(0).astype(int)\n",
    "perc = (counts / counts.sum() * 100).round(1)\n",
    "\n",
    "print('\\nSample counts:')\n",
    "for label, value in counts.items():\n",
    "    print(f\"  {label}: {value} ({perc[label]}%)\")\n",
    "\n",
    "sample_out = 'sample_200.csv'\n",
    "sample_df.to_csv(sample_out, index=False)\n",
    "print(f\"Saved sample to {sample_out}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "ax = counts.plot(kind='bar', color=['#d62728','#1f77b4'])\n",
    "plt.title(f'Dehumanization distribution in random sample (n={n_take})')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{int(p.get_height())}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nFirst 5 rows of the sample:')\n",
    "cols_to_show = [c for c in ['instance_id','displayed_text','exp_dehum_binary'] if c in sample_df.columns]\n",
    "print(sample_df.head()[cols_to_show].fillna('').to_string(index=False))\n"
   ],
   "id": "e6369767b8e1af83"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
